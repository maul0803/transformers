{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nano_graphrag import GraphRAG, QueryParam\n",
    "from nano_graphrag.base import BaseKVStorage\n",
    "from nano_graphrag._utils import compute_args_hash, wrap_embedding_func_with_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RuntimeError: This event loop is already running\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup logging ===\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger(\"nano-graphrag\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"./nano_graphrag_cache_flanT5\"\n",
    "cache_dir = \"/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/transformers/GRAPH_RAG2/cache\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# === Load embedding model ===\n",
    "EMBED_MODEL = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=WORKING_DIR, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_embedding_func_with_attrs(\n",
    "    embedding_dim=EMBED_MODEL.get_sentence_embedding_dimension(),\n",
    "    max_token_size=EMBED_MODEL.max_seq_length,\n",
    ")\n",
    "async def local_embedding(texts: list[str]) -> np.ndarray:\n",
    "    return EMBED_MODEL.encode(texts, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "llm_model_name = \"google/flan-t5-base\"\n",
    "#llm_model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "#llm_model_name = \"facebook/bart-large\"\n",
    "#llm_model_name = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "#llm_tokenizer_name = \"Qwen/Qwen2.5-7B\"\n",
    "#llm_model_name = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "llm_tokenizer_name = \"INSAIT-Institute/BgGPT-Gemma-2-27B-IT-v1.0\"\n",
    "llm_model_name = \"INSAIT-Institute/BgGPT-Gemma-2-27B-IT-v1.0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print('DEVICE:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac130dd2948f4ba7a1de6efe9e9fc521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784dd243ac494d2ca24f0f4a3230efaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a968183f95d749a28d55a186d0ba91d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a849af83e7242558e17bf5d9f49b15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924288481fa249f8a7a3a69962443d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/853 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880af5ebe17d4970a025bc8c47992b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/42.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b949fd2705cb44ff84cf9bbe5339e666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69073dc202e94e90a7a893bb79cbd79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00012.safetensors:   0%|          | 0.00/4.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d809d7f8a94f47eeacd94566933a3978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dfa1017b7b4cdfb08bf0d4772e7f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44401d1737647028d9c2f842a015c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294a4cf87c3f490c988b0e8ca360ed9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a649fba8144a4095b5cb3a698482fd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ac6bd90df042d8b6bc5ac6f193b9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829355da85d44eed971b829da9dfcdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424526b3364f4dcc8ff1c164b227c295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c481fa9266384ac2b1ba9533c0e17d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a8523548f34e9dbb2a140f459e99d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29895ab7be14913ab206a5b70b5d717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00012.safetensors:   0%|          | 0.00/680M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182f1caa69c648daac7e85716a0e43a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === Load the Seq2Seq LLM ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_name, cache_dir = cache_dir)\n",
    "#llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name, cache_dir = cache_dir)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name, cache_dir = cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: gravity is the motion of a solid or solid object in the Earth's orbit.\n"
     ]
    }
   ],
   "source": [
    "# test of the model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "llm_model = llm_model.to(DEVICE)\n",
    "\n",
    "prompt = \"Can you explain gravity in simple terms?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        num_beams=1\n",
    "    )\n",
    "\n",
    "result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"LLM Response:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def my_llm_complete(\n",
    "    prompt, system_prompt=None, history_messages=[], **kwargs\n",
    ") -> str:\n",
    "    input_text = prompt if not system_prompt else f\"{system_prompt}\\n{prompt}\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "        )\n",
    "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(\"🧪 LLM output:\", output_text)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 LLM output: Albert Einstein , Birth Date\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Albert Einstein , Birth Date'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Please extract the named entities and their types (Person, Location, Date, etc.) from the following text: 'Albert Einstein was born in Ulm, Germany, in 1879. He is known for developing the theory of relativity.' Return the result in the following format:[ {'entity': 'Albert Einstein', 'type': 'Person'}, {'entity': 'Ulm', 'type': 'Location'}, {'entity': 'Germany', 'type': 'Location'}, {'entity': '1879', 'type': 'Date'} ]\"\n",
    "await my_llm_complete(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Utility ===\n",
    "def remove_if_exist(file):\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag(enable_cache=False):\n",
    "    return GraphRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        enable_llm_cache=enable_cache,\n",
    "        best_model_func=my_llm_complete,\n",
    "        cheap_model_func=my_llm_complete,\n",
    "        embedding_func=local_embedding,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main functions ===\n",
    "def query(question=\"What is X\"):\n",
    "    rag = get_rag()\n",
    "    print(\n",
    "        rag.query(\n",
    "            question, param=QueryParam(mode=\"global\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert():\n",
    "    from time import time\n",
    "\n",
    "    with open(\"./dataset/test.txt\", encoding=\"utf-8-sig\") as f:\n",
    "        FAKE_TEXT = f.read()\n",
    "\n",
    "    remove_if_exist(f\"{WORKING_DIR}/vdb_entities.json\")\n",
    "    remove_if_exist(f\"{WORKING_DIR}/kv_store_full_docs.json\")\n",
    "    remove_if_exist(f\"{WORKING_DIR}/kv_store_text_chunks.json\")\n",
    "    remove_if_exist(f\"{WORKING_DIR}/kv_store_community_reports.json\")\n",
    "    remove_if_exist(f\"{WORKING_DIR}/graph_chunk_entity_relation.graphml\")\n",
    "\n",
    "    rag = get_rag()\n",
    "    start = time()\n",
    "    rag.insert(FAKE_TEXT)\n",
    "    print(\"indexing time:\", time() - start)\n",
    "    return FAKE_TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:nano-graphrag:GraphRAG init with param:\n",
      "\n",
      "  working_dir = ./nano_graphrag_cache_flanT5,\n",
      "  enable_local = True,\n",
      "  enable_naive_rag = False,\n",
      "  chunk_func = <function chunking_by_token_size at 0x1521626320e0>,\n",
      "  chunk_token_size = 1200,\n",
      "  chunk_overlap_token_size = 100,\n",
      "  tiktoken_model_name = gpt-4o,\n",
      "  entity_extract_max_gleaning = 1,\n",
      "  entity_summary_to_max_tokens = 500,\n",
      "  graph_cluster_algorithm = leiden,\n",
      "  max_graph_cluster_size = 10,\n",
      "  graph_cluster_seed = 3735928559,\n",
      "  node_embedding_algorithm = node2vec,\n",
      "  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n",
      "  special_community_report_llm_kwargs = {'response_format': {'type': 'json_object'}},\n",
      "  embedding_func = {'embedding_dim': 384, 'max_token_size': 256, 'func': <function local_embedding at 0x15216004c280>},\n",
      "  embedding_batch_num = 32,\n",
      "  embedding_func_max_async = 16,\n",
      "  query_better_than_threshold = 0.2,\n",
      "  using_azure_openai = False,\n",
      "  best_model_func = <function my_llm_complete at 0x15215f52a0e0>,\n",
      "  best_model_max_token_size = 32768,\n",
      "  best_model_max_async = 16,\n",
      "  cheap_model_func = <function my_llm_complete at 0x15215f52a0e0>,\n",
      "  cheap_model_max_token_size = 32768,\n",
      "  cheap_model_max_async = 16,\n",
      "  entity_extraction_func = <function extract_entities at 0x1521626327a0>,\n",
      "  key_string_value_json_storage_cls = <class 'nano_graphrag._storage.kv_json.JsonKVStorage'>,\n",
      "  vector_db_storage_cls = <class 'nano_graphrag._storage.vdb_nanovectordb.NanoVectorDBStorage'>,\n",
      "  vector_db_storage_cls_kwargs = {},\n",
      "  graph_storage_cls = <class 'nano_graphrag._storage.gdb_networkx.NetworkXStorage'>,\n",
      "  enable_llm_cache = False,\n",
      "  always_create_working_dir = True,\n",
      "  addon_params = {},\n",
      "  convert_response_to_json_func = <function convert_response_to_json at 0x15216280a710>\n",
      "\n",
      "INFO:nano-graphrag:Load KV full_docs with 0 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 0 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './nano_graphrag_cache_flanT5/vdb_entities.json'} 0 data\n",
      "INFO:nano-graphrag:[New Docs] inserting 1 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 2 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 LLM output: Alex clenched his jaw, buzz of frustration, Jordan , Jordan\n",
      "🧪 LLM output: MANY entities were missed in the last extraction.\n",
      "🧪 LLM output: Alex clenched his jaw, buzz of frustration, Jordan , Jordan\n",
      "🧪 LLM output: MANY entities were missed in the last extraction.\n",
      "⠹ Processed 2 chunks, 0 entities(duplicated), 0 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nano-graphrag:Didn't extract any entities, maybe your LLM is not working\n",
      "WARNING:nano-graphrag:No new entities found\n",
      "INFO:nano-graphrag:Writing graph with 0 nodes, 0 edges\n",
      "DEBUG:nano-graphrag:GraphRAG init with param:\n",
      "\n",
      "  working_dir = ./nano_graphrag_cache_flanT5,\n",
      "  enable_local = True,\n",
      "  enable_naive_rag = False,\n",
      "  chunk_func = <function chunking_by_token_size at 0x1521626320e0>,\n",
      "  chunk_token_size = 1200,\n",
      "  chunk_overlap_token_size = 100,\n",
      "  tiktoken_model_name = gpt-4o,\n",
      "  entity_extract_max_gleaning = 1,\n",
      "  entity_summary_to_max_tokens = 500,\n",
      "  graph_cluster_algorithm = leiden,\n",
      "  max_graph_cluster_size = 10,\n",
      "  graph_cluster_seed = 3735928559,\n",
      "  node_embedding_algorithm = node2vec,\n",
      "  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n",
      "  special_community_report_llm_kwargs = {'response_format': {'type': 'json_object'}},\n",
      "  embedding_func = {'embedding_dim': 384, 'max_token_size': 256, 'func': <function local_embedding at 0x15216004c280>},\n",
      "  embedding_batch_num = 32,\n",
      "  embedding_func_max_async = 16,\n",
      "  query_better_than_threshold = 0.2,\n",
      "  using_azure_openai = False,\n",
      "  best_model_func = <function my_llm_complete at 0x15215f52a0e0>,\n",
      "  best_model_max_token_size = 32768,\n",
      "  best_model_max_async = 16,\n",
      "  cheap_model_func = <function my_llm_complete at 0x15215f52a0e0>,\n",
      "  cheap_model_max_token_size = 32768,\n",
      "  cheap_model_max_async = 16,\n",
      "  entity_extraction_func = <function extract_entities at 0x1521626327a0>,\n",
      "  key_string_value_json_storage_cls = <class 'nano_graphrag._storage.kv_json.JsonKVStorage'>,\n",
      "  vector_db_storage_cls = <class 'nano_graphrag._storage.vdb_nanovectordb.NanoVectorDBStorage'>,\n",
      "  vector_db_storage_cls_kwargs = {},\n",
      "  graph_storage_cls = <class 'nano_graphrag._storage.gdb_networkx.NetworkXStorage'>,\n",
      "  enable_llm_cache = False,\n",
      "  always_create_working_dir = True,\n",
      "  addon_params = {},\n",
      "  convert_response_to_json_func = <function convert_response_to_json at 0x15216280a710>\n",
      "\n",
      "INFO:nano-graphrag:Load KV full_docs with 0 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 0 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 0 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "indexing time: 1.455812692642212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Loaded graph from ./nano_graphrag_cache_flanT5/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\n",
      "INFO:nano-vectordb:Load (0, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './nano_graphrag_cache_flanT5/vdb_entities.json'} 0 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I'm not able to provide an answer to that question.\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"nano-graphrag\").setLevel(logging.DEBUG)\n",
    "# === Entry point ===\n",
    "if __name__ == \"__main__\":\n",
    "    FAKE_TEXT = insert()\n",
    "    query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X is a person who often goes unnoticed at first glance. There’s nothing about them that demands immediate attention, no loud or flashy traits to captivate the eye. Yet, in the moments that X does speak, or when they do step forward, something about them lingers in the air, a subtle yet undeniable presence. It's not that they are intentionally mysterious or reserved, but more that X carries a quiet, understated strength that others find both intriguing and comforting.\\nPeople who know X often describe them as a paradox: someone who can be both invisible and omnipresent at the same time. In a crowded room, X might seem to fade into the background, their demeanor calm, unassuming. But even in their silence, there's something magnetic about them—an energy that others can't quite place. It isn’t brash or loud; it’s almost as though they are simply so in tune with the world around them that it becomes impossible to ignore them once you notice their subtle influence.\\nX’s way of moving through life is graceful, deliberate, and slow. They have an unhurried quality to them, a sort of ease that allows them to fully engage with the present moment. This is not to say that X is lazy or disinterested; far from it. Instead, it’s as if they understand that the more you rush through life, the more you miss. They approach everything they do with patience, allowing each task, conversation, and experience to unfold naturally, without forcing anything. This sense of calm is contagious. Others find themselves slowing down when in X's presence, becoming more mindful of the details they might otherwise overlook.\\nYet, despite their calm exterior, X is not without depth. Beneath the surface of serenity lies a mind that is constantly processing, observing, and thinking. X is deeply introspective, someone who reflects on their experiences and those of others. They are the type of person who finds meaning in the small things—the way the light falls on a table, the rhythm of a conversation, or the fleeting look in someone’s eyes. X doesn’t rush to fill silence with noise; instead, they let it linger, letting each moment breathe.\\nOne of X’s most striking qualities is their ability to listen. In a world where many are quick to speak and slow to listen, X stands as a rare example of true attentiveness. They don’t just hear words; they listen to what is behind them—the emotions, the intentions, the unspoken thoughts. When someone speaks to X, they feel heard in a way that is almost therapeutic. X doesn’t interrupt or offer quick solutions; instead, they simply create a space where the other person can be themselves, where their feelings can be expressed without judgment. This listening ability has made X someone people turn to when they need a safe space to vent, to cry, or simply to think things through aloud.\\nX’s friendships are deep and meaningful. While they may not have a large circle of acquaintances, the relationships they maintain are founded on trust, respect, and genuine connection. They are not one for small talk, preferring instead to engage in conversations that explore the complexities of life, the beauty of art, or the intricacies of human nature. X doesn’t need to be surrounded by people to feel fulfilled. In fact, they often find solace in solitude, using their alone time to recharge and reflect.\\nHowever, this doesn’t mean that X is disconnected from the world. On the contrary, they have an innate ability to connect with others on a level that feels rare and authentic. People who meet X often walk away feeling as though they have had an encounter with someone truly unique—someone who sees them, not just their exterior, but who they are at their core. This ability to connect makes X an incredible friend, confidant, and even mentor. They never push others to share, but when someone opens up to them, they do so with a quiet assurance that X will understand.\\nIn their professional life, X approaches their work with the same calm determination that defines their personal life. They are meticulous, focused, and driven, yet never rushed. X isn’t the type to boast about their achievements or seek recognition. Instead, they find fulfillment in the work itself—the process, the challenge, the satisfaction of creating something meaningful. They thrive in environments where their ability to listen, observe, and analyze is valued, and where they can work at their own pace, contributing to a larger goal.\\nThough X might not seek out leadership roles, they have a natural ability to inspire others. This comes not from a place of authority or command but from their integrity and example. X leads by doing, showing others how to approach challenges with grace and how to value the process over the outcome. People follow X not because they have to, but because they want to. X doesn’t lead with words or directives; they lead with their actions, and in doing so, they earn the respect and admiration of those around them.\\nIn relationships, X is fiercely loyal and supportive. They have a quiet way of showing care, often through small gestures that speak volumes. Whether it’s a simple check-in message, a thoughtful note, or just being present when needed, X shows up in ways that feel meaningful. They are the kind of person who remembers the small details that others might forget—the favorite book of a friend, the anniversary of a shared experience, or a quiet moment in time that left a lasting impression. They don’t need to make grand gestures to prove their love or commitment. Instead, they demonstrate it through consistency, presence, and a deep understanding of the people in their lives.\\nYet, despite their many admirable qualities, X is not without their own struggles. Like everyone, they face moments of self-doubt, times when the world feels overwhelming or when their quiet nature leads to feelings of isolation. However, X handles these moments with the same grace that defines their life. Rather than seeking attention or sympathy, they retreat inward, reflecting on their feelings and finding ways to heal in solitude. X doesn’t seek perfection, but they do seek understanding—of themselves and the world around them.\\nIn many ways, X is a quiet observer of life. They see the beauty in the mundane, the poetry in the ordinary, and the lessons in the challenges. They don’t need to be the loudest voice in the room because they know that sometimes, the most profound moments come in the silence. X’s life is a testament to the power of presence—the ability to simply be, to observe, and to connect in ways that are often missed by others. They remind those around them that there is more to life than the noise, and that true connection is often found in the quietest of moments.\\nAs they move through life, X leaves behind a trail of calm, a ripple effect that touches the lives of those fortunate enough to cross their path. They may never seek fame or recognition, but they have the rare gift of making those around them feel valued and understood. In a world that often feels chaotic and loud, X’s quiet strength serves as a reminder of the power of simplicity, the importance of listening, and the beauty of being present.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FAKE_TEXT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers3",
   "language": "python",
   "name": "transformers3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
