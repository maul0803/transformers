{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Charger et parser les fichiers PDF ===\n",
    "def load_pdfs(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            reader = PdfReader(os.path.join(directory, filename))\n",
    "            text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "            documents.append({\"id\": str(uuid4()), \"text\": text})\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Split des documents (simple) ===\n",
    "def split_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Générer les embeddings via l'API OpenAI ===\n",
    "def get_embeddings(texts, client, embedding_model_name):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), 100):  # batcher par 100 max\n",
    "        batch = texts[i:i+100]\n",
    "        response = client.embeddings.create(\n",
    "            model=embedding_model_name,\n",
    "            input=batch\n",
    "        )\n",
    "        for d in response.data:\n",
    "            embeddings.append(np.array(d.embedding, dtype=np.float32))\n",
    "    return np.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# === 4. Indexer avec FAISS ===\n",
    "def build_faiss_index(chunks, client, embedding_model_name):\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    vectors = get_embeddings(texts, client, embedding_model_name)\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(vectors)\n",
    "    return index, vectors, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Récupérer les documents pertinents ===\n",
    "def retrieve(query, index, texts, client, embedding_model_name, top_k=4):\n",
    "    query_embedding = client.embeddings.create(\n",
    "        model=embedding_model_name,\n",
    "        input=[query]\n",
    "    ).data[0].embedding\n",
    "    query_vector = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    return [texts[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Réponse générée :\n",
      " World domination RISK\n"
     ]
    }
   ],
   "source": [
    "# === 6. Appeler le modèle LLM ===\n",
    "def ask_llm(question, context, client, llm_model_name):\n",
    "    prompt = f\"\"\"Answer the following question based on the provided context.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0. Initialisation du client OpenAI ===\n",
    "client = OpenAI()\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-small\"   # Ton modèle d'embedding\n",
    "llm_model_name = \"gpt-4.1\"                        # Ton modèle LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"dataset\"\n",
    "documents = load_pdfs(pdf_dir)\n",
    "\n",
    "# Split en chunks\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    for chunk in split_text(doc[\"text\"]):\n",
    "        chunks.append({\"id\": doc[\"id\"], \"text\": chunk})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index FAISS\n",
    "index, vectors, texts = build_faiss_index(chunks, client, embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "question = \"What is the name of the game?\"\n",
    "top_chunks = retrieve(question, index, texts, client, embedding_model_name, top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "tournament, write to us at the address below.\n",
      "We will be happy to answer questions about this game. Write: Consumer\n",
      "Relations Department, Parker Brothers, P.O. Box 1012, Beverly, MA 01915.\n",
      "“F!HPARKERBROTHERS\n",
      "00044-I \n",
      "Rl\n",
      "16\n",
      "\n",
      "--- Document 2 ---\n",
      "WORLD DOMINATION RISK®\n",
      "OBJECT OF THE GAME\n",
      "To conquer the world by occupying every territory on the board, thus\n",
      "eliminating all your opponents.\n",
      "SETUP\n",
      "Unlike most games, RISK demands careful planning before you actually\n",
      "start to play. This Initial Army Placement sets the stage for the battles you ’ll\n",
      "fight later on.\n",
      "INITIAL ARMY PLACEMENT  consists of these steps:\n",
      "1.\n",
      "2.\n",
      "3.\n",
      "4.\n",
      "Select a color and, depending on the number of players, count out the\n",
      "“ armies” you ’ll need to start the game.\n",
      "If 2 are playing, see instructions on page 11.\n",
      "If 3 are playing, each player counts out 35 Infantry.\n",
      "If 4 are playing, each player counts out 30 Infantry.\n",
      "If 5 are playing, each player counts out 25 Infantry.\n",
      "If 6 are playing, each player counts out 20 Infantry.\n",
      "Roll one die. Whoever rolls the highest number takes one Infantry piece\n",
      "from his or her pile and places it onto any territory on the board, thus\n",
      "claiming that territory.\n",
      "Starting to the left of the first player, everyone in turn places one army\n",
      "\n",
      "--- Document 3 ---\n",
      "Fortify borders adjacent to enemy territories for better defense if a\n",
      "neighbor decides to attack you.\n",
      "EQUIPMENT\n",
      "1 Tri-fold Game Boar d l 5 Dice: 2 white and 3 re d l Deck of 56 RISK cards\n",
      "l 6 Sets of armies, each a different color\n",
      "The Game Board. The game board is a map of 6 continents divided into\n",
      "42 territories. Each continent is a different color and contains from 4 to  12\n",
      "territories. The numbers along the bottom (southern) edge of the board\n",
      "indicate the number of armies you will receive for a set of cards you trade\n",
      "in, as explained on page 7.\n",
      "\n",
      "--- Document 4 ---\n",
      "TABLE OF CONTENTS\n",
      "Introduction & Strategy Hint s ....................................................................... 3\n",
      "Equipment ....................................................................................................... 3\n",
      "RISK®, the classic WORLD DOMINATION game ........................ 5\n",
      "Setup, including initial placement of armie s .................................. 5\n",
      "Playing ................................................................................................ .6\n",
      "Getting and placing new armie s ....................................................... 6\n",
      "RISK cards ............................................................................................ 7\n",
      "Attacking ........................................................................................... . 8\n",
      "Determining the winner of the dice rol l .......................................... 9\n",
      "Fortifying your positio n ................................................................... 10\n"
     ]
    }
   ],
   "source": [
    "# Générer réponse\n",
    "context = \"\\n\\n\".join(top_chunks)\n",
    "answer = ask_llm(question, context, client, llm_model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 Question :\", question)\n",
    "print(\"\\n🧠 Réponse générée :\\n\", answer)\n",
    "\n",
    "# Afficher les passages utilisés\n",
    "print(\"\\n--- Passages utilisés ---\")\n",
    "for i, passage in enumerate(top_chunks):\n",
    "    print(f\"\\n[{i+1}] {passage[:300]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers3",
   "language": "python",
   "name": "transformers3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
